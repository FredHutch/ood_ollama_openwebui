#!/usr/bin/env bash
module purge

# load RStudio Server # uv
module load Apptainer uv

# Output debug info
module list



  

echo port is $port
echo host is $host

echo
echo NOT USING THESE now:
echo webui_port is $webui_port
echo ollama_port is $ollama_port
echo ------


# excerpt from https://discourse.openondemand.org/t/avoid-launching-a-web-browser/4060/14?u=dtenenba


<%

nv = ""
if context.want_gpu == "1"
  nv = " --nv "
end


%>


set -x
set -e

export OLLAMA_HOST=${host}:11434
export OLLAMA_MODELS=/app/ollama/models
export OLLAMA_CHAT_HOST=0.0.0.0
export OLLAMA_CHAT_URL_PREFIX=/node/${host}/${port}
echo "OLLAMA_CHAT_URL_PREFIX=${OLLAMA_CHAT_URL_PREFIX}"
apptainer run  <%= nv %>  --env OLLAMA_HOST=$localhost:11434 -B /app/ollama/models:/app/ollama/models:ro --writable-tmpfs --env OLLAMA_MODELS=${OLLAMA_MODELS} https://sif-registry.fredhutch.org/ollama_latest.sif  serve  > /dev/null 2>&1 &


#uv run --with git+https://github.com/FredHutch/ollama-chat.git ollama-chat -p ${port} -x -n
# cd ~/dev/ollama-chat
# source .venv/bin/activate
# ollama-chat

echo we bout to do this
/home/dtenenba/dev/ollama-chat/.venv/bin/ollama-chat -p ${port} -n -x

echo we did that


# apptainer run --writable-tmpfs \
#      --env FORWARD_PORT=${port} \
#      --env FORWARD_HOST=${host} \
#      -B ./templates:/etc/nginx/templates \
#      -B ./:/var/log/nginx/ \
#      -B ./conf:/etc/nginx/conf.d/ \
#      -B ./client_temp:/var/cache/nginx/client_temp/ \
#      -B ./var_run:/var/run/ \
#      --app docker \
#      https://sif-registry.fredhutch.org/nginx_1.26.3.sif nginx

